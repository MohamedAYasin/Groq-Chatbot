{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCIo4Cb6z5i7"
      },
      "source": [
        "# Introduction to LLMs: Build a Simple Chatbot\n",
        "\n",
        "# In this notebook, you'll learn:\n",
        "\n",
        "*   What LLMs are and how to use them\n",
        "*   How to use the Groq API to build a chatbot\n",
        "*   Key concepts like temperature and tokens\n",
        "\n",
        "\n",
        "# Let's get started!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAGi7To20DAY",
        "outputId": "38f12fda-34d8-4dc7-c577-0c2e7bf88e70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, groq\n",
            "Successfully installed groq-0.11.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2\n"
          ]
        }
      ],
      "source": [
        "# First, install the Groq Python client\n",
        "!pip install groq\n",
        "\n",
        "# Import necessary libraries\n",
        "import os  # Used for environment variables\n",
        "from groq import Groq # Groq client to interact with the API\n",
        "\n",
        "# os: Helps manage API keys securely using environment variables.\n",
        "# groq: A client that allows us to send prompts to Groq’s LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KocXSOna20Xy"
      },
      "outputs": [],
      "source": [
        "# Set your Groq API key\n",
        "api_key = \"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\"  # Replace with your actual Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = api_key  # Store it as an environment variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQY-AKPfA8d2"
      },
      "outputs": [],
      "source": [
        "# Initialize the Groq client using the stored API key\n",
        "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "# This initializes the client object, which allows us to send requests to the API. This is kinda like \"logging in\" with the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lWZD7ZSkBXEz",
        "outputId": "609c518c-5fcf-412d-8cf1-9fc874644378"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nFunction Breakdown:\\nThis function takes a prompt (what we ask the model) and sends it to Groq’s Llama-based LLM.\\nmax_tokens: Sets the limit for how much text the model can return.\\ntemperature: Affects how random or creative the responses will be (higher = more creative).\\nHandling Errors: This code catches common errors (like an invalid API key) and prints a helpful message.\\n\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a function to generate text\n",
        "def generate_text(prompt, max_tokens=100):\n",
        "    try:\n",
        "        # Send a prompt to Groq's LLM\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            model=\"llama3-8b-8192\",  # Llama model to generate responses\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=0.7  # Controls randomness\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content  # Extract the response\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        if \"Invalid API Key\" in str(e):\n",
        "            print(\"Please check that you've entered your API key correctly.\")\n",
        "        return None\n",
        "\n",
        "\"\"\"\n",
        "Function Breakdown:\n",
        "This function takes a prompt (what we ask the model) and sends it to Groq’s Llama-based LLM.\n",
        "max_tokens: Sets the limit for how much text the model can return.\n",
        "temperature: Affects how random or creative the responses will be (higher = more creative).\n",
        "Handling Errors: This code catches common errors (like an invalid API key) and prints a helpful message.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZwYID_8BujK",
        "outputId": "604b0b78-52a9-413f-a287-e8e2fa6e8d85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Who is the most influential person in human history? gimme top three\n",
            "\n",
            "Generated text: What a great question! Identifying the most influential person in human history is a challenging task, as it's a subjective evaluation that can vary depending on individual perspectives and criteria. However, based on various sources, including historians, philosophers, and scholars, here are my top three picks for the most influential person in human history:\n",
            "\n",
            "1. **Jesus Christ** (c. 4 BC - 30 AD):\n",
            "Jesus is widely regarded as one of the most influential figures in human history. His teachings,\n"
          ]
        }
      ],
      "source": [
        "# Example usage of the chatbot\n",
        "prompt = \"Who is the most influential person in human history? gimme top three\"\n",
        "generated_text = generate_text(prompt)\n",
        "\n",
        "if generated_text:\n",
        "    print(f\"Prompt: {prompt}\\n\")\n",
        "    print(f\"Generated text: {generated_text}\")\n",
        "else:\n",
        "    print(\"Failed to generate text. Please check your API key and permissions.\")\n",
        "\n",
        "# Try changing the prompt to something new (e.g., \"What is Python programming?\").\n",
        "# Adjust max_tokens to control how long the response is.\n",
        "# Experiment with different temperature values (between 0 and 1) to see how the model’s behavior changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA2y2OxeCQ6F",
        "outputId": "de271c07-4807-461f-ce74-1ed6b3d3683a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Groq-powered chatbot! Type 'quit' to exit.\n",
            "Bot: A salve is a type of topical medication or ointment that is used to treat various skin conditions, such as wounds, burns, cuts, and abrasions. It is typically a semi-solid preparation that is applied directly to the skin to provide a protective barrier and promote healing.\n",
            "\n",
            "Salves can be made from a variety of ingredients, including herbs, plant extracts, oils, and other natural substances. They may be used to:\n",
            "\n",
            "1. Soothe and calm irritated skin\n",
            "2. Reduce\n",
            "Bot: A classic Italian greeting!\n",
            "\n",
            "\"Ciao\" (pronounced \"CHOW\") is an Italian word that can be used as both a formal and informal greeting, similar to \"hello\" or \"hi\" in English. It's a versatile word that can be used in various situations, such as:\n",
            "\n",
            "1. **Formal greeting**: When used in formal situations, like in business or with people you don't know well, \"Ciao\" is a polite way to greet someone.\n",
            "Example: \"C\n",
            "Bot: I'm just an AI, I don't have access to real-time information, so I don't know what the current time is. I can suggest some ways for you to find out the current time, though!\n",
            "\n",
            "You can:\n",
            "\n",
            "1. Check your phone or watch: If you have a phone or watch with you, you can check the time on the screen.\n",
            "2. Ask a virtual assistant: If you have a virtual assistant like Alexa, Google Assistant, or Siri, you can ask them what\n",
            "Bot: LLaMA (Large Language Model Application) is a large language model developed by Meta AI, and it shares many similarities with ChatGPT (Chat Generative Pre-Training Transformer). Both models are based on transformer architecture and have been trained on massive amounts of text data. Here are some key similarities:\n",
            "\n",
            "1. **Transformer architecture**: Both LLaMA and ChatGPT are built on top of the transformer architecture, which is a type of recurrent neural network (RNN) designed specifically for sequential\n",
            "Bot: Large Language Models (LLMs) are a type of artificial intelligence (AI) model that is specifically designed to process and generate human-like language. They are a subset of Natural Language Processing (NLP) models, which are designed to analyze, understand, and generate human language.\n",
            "\n",
            "LLMs are typically trained on massive amounts of text data, which enables them to learn patterns, relationships, and structures of language. These models are characterized by their large size, typically measured in millions or even billions of parameters\n"
          ]
        }
      ],
      "source": [
        "# Chatbot Simulation Loop\n",
        "print(\"Welcome to the Groq-powered chatbot! Type 'quit' to exit.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    response = generate_text(user_input)\n",
        "    print(f\"Bot: {response}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}